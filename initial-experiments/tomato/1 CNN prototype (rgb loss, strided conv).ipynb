{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6541a7e2-a393-4b43-8c9e-fed38b683802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c69d6788-f498-4f0b-8618-10eddceea748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss function, taking into account the RGB values\n",
    "# 효과는 크게 없었지만, 논리적으로는 맞는 부분인 것 같음.\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Custom loss function that applies MSE specifically to RGB channels\n",
    "def mse_rgb(y_true, y_pred):\n",
    "    # y_true and y_pred are assumed to be in shape (batch_size, height, width, 3)\n",
    "    return K.mean(K.square(y_true - y_pred), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2228fdc-0e0a-4247-9870-5b41804bacc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more pooling with stride instead of GAP2D\n",
    "\n",
    "def build_model(input_shape, output_frames):\n",
    "    \"\"\"\n",
    "    Builds a CNN-based encoder-decoder model for animation frame prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    input_shape (tuple): The shape of the input frame (height, width, channels).\n",
    "    output_frames (int): The number of frames to generate as output.\n",
    "    \n",
    "    Returns:\n",
    "    model: The compiled Keras model.\n",
    "    \"\"\"\n",
    "    # Encoder\n",
    "    encoder_input = layers.Input(shape=input_shape, name='encoder_input')\n",
    "    \n",
    "    # CNN layers to encode the first frame\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(encoder_input)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(256, (3, 3), strides=2, padding='same', activation='relu')(x)\n",
    "    \n",
    "    # Flatten the output of the convolutional layers\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    # Latent representation\n",
    "    latent = layers.Dense(512, activation='relu', name='latent_vector')(x)\n",
    "\n",
    "    # Decoder (generate multiple frames)\n",
    "    decoder_output = layers.Dense(output_frames * input_shape[0] * input_shape[1] * input_shape[2], activation='sigmoid')(latent)\n",
    "    \n",
    "    # Reshape to the desired output shape (frames, height, width, channels)\n",
    "    decoder_output = layers.Reshape((output_frames, input_shape[0], input_shape[1], input_shape[2]), name='decoder_output')(decoder_output)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = models.Model(encoder_input, decoder_output, name='frame_predictor_model')\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=mse_rgb, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e690ac98-4390-4e58-af74-46254e120246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # original/flatten\n",
    "\n",
    "# def build_model(input_shape, output_frames):\n",
    "#     \"\"\"\n",
    "#     Builds a CNN-based encoder-decoder model for animation frame prediction.\n",
    "    \n",
    "#     Parameters:\n",
    "#     input_shape (tuple): The shape of the input frame (height, width, channels).\n",
    "#     output_frames (int): The number of frames to generate as output.\n",
    "    \n",
    "#     Returns:\n",
    "#     model: The compiled Keras model.\n",
    "#     \"\"\"\n",
    "#     # Encoder\n",
    "#     encoder_input = layers.Input(shape=input_shape, name='encoder_input')\n",
    "    \n",
    "#     # CNN layers to encode the first frame\n",
    "#     x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(encoder_input)\n",
    "#     x = layers.MaxPooling2D((2, 2))(x)\n",
    "#     x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "#     x = layers.MaxPooling2D((2, 2))(x)\n",
    "#     x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "#     # x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "#     # # Latent representation\n",
    "#     # latent = layers.Dense(512, activation='relu', name='latent_vector')(x)\n",
    "#     x = layers.Flatten()(x)  # Flatten instead of GlobalAveragePooling\n",
    "#     latent = layers.Dense(512, activation='relu', name='latent_vector')(x)\n",
    "\n",
    "#     # Decoder (generate multiple frames)\n",
    "#     decoder_output = layers.Dense(output_frames * input_shape[0] * input_shape[1] * input_shape[2], activation='sigmoid')(latent)\n",
    "#     decoder_output = layers.Reshape((output_frames, input_shape[0], input_shape[1], input_shape[2]), name='decoder_output')(decoder_output)\n",
    "\n",
    "#     # Build and compile the model\n",
    "#     model = models.Model(encoder_input, decoder_output, name='frame_predictor_model')\n",
    "    \n",
    "#     model.compile(optimizer='adam', loss=mse_rgb, metrics=['mae'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c20e4a1f-7a12-4a20-a48e-9f2a9a14b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sprite_data(sprite_sheet_paths, num_frames):\n",
    "    \"\"\"\n",
    "    Processes sprite sheets and returns animation data without saving individual images.\n",
    "    \n",
    "    Parameters:\n",
    "    sprite_sheet_paths (list): List of paths to the sprite sheet images.\n",
    "    num_frames (int): Total number of frames (sprites) in the sprite sheet.\n",
    "    \n",
    "    Returns:\n",
    "    X (numpy array): Array of input frames (first frame of each animation sequence).\n",
    "    Y (numpy array): Array of output frames (remaining frames of each animation sequence).\n",
    "    \"\"\"\n",
    "    # Initialize lists for input (X) and output (Y) frames\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    # Process each sprite sheet in the provided paths\n",
    "    for i in range(len(sprite_sheet_paths)):\n",
    "        sprite_sheet = Image.open(sprite_sheet_paths[i])\n",
    "        \n",
    "        # Get sprite sheet dimensions\n",
    "        sheet_width, sheet_height = sprite_sheet.size\n",
    "        \n",
    "        # Calculate the width and height of each frame\n",
    "        frame_width = sheet_width // num_frames\n",
    "        frame_height = sheet_height  # Height of each frame is the same as the height of the sprite sheet\n",
    "        \n",
    "        # Loop through each frame in the sprite sheet\n",
    "        for frame_num in range(num_frames):\n",
    "            # Calculate the coordinates of the sprite in the sprite sheet\n",
    "            left = frame_num * frame_width\n",
    "            upper = 0\n",
    "            right = left + frame_width\n",
    "            lower = frame_height\n",
    "            \n",
    "            # Crop the sprite from the sprite sheet\n",
    "            frame = sprite_sheet.crop((left, upper, right, lower))\n",
    "            \n",
    "            # Resize to 128x128 if needed and convert to RGB\n",
    "            frame = frame.convert('RGB').resize((128, 128))\n",
    "            \n",
    "            # Normalize pixel values to [0, 1]\n",
    "            frame_array = np.array(frame) / 255.0\n",
    "            \n",
    "            # Append the first frame to X and the remaining ones to Y\n",
    "            if frame_num == 0:\n",
    "                X.append(frame_array)  # The first frame goes into X\n",
    "            else:\n",
    "                if len(Y) <= i:  # Initialize the sublist for Y if necessary\n",
    "                    Y.append([])\n",
    "                Y[i].append(frame_array)  # Subsequent frames go into Y\n",
    "    \n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af095657-3f67-4b2a-a146-cd716c3624dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_frames(input_frame, output_frames):\n",
    "    \"\"\"\n",
    "    Merge input_frame (1, 128, 128, 3) and output_frames (9, 128, 128, 3) into one array.\n",
    "    \n",
    "    Parameters:\n",
    "    input_frame (numpy array): The first frame with shape (1, 128, 128, 3).\n",
    "    output_frames (numpy array): The rest of the frames with shape (9, 128, 128, 3).\n",
    "\n",
    "    Returns:\n",
    "    numpy array: Merged frames with shape (10, 128, 128, 3).\n",
    "    \"\"\"    \n",
    "    # Concatenate the input frame with the output frames\n",
    "    merged_frames = np.concatenate(([input_frame], output_frames), axis=0)\n",
    "    \n",
    "    return merged_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c06fbf34-d1b9-4ce4-9259-feb417f3a599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_predicted_frames(predicted_frames):\n",
    "    \"\"\"\n",
    "    Display predicted frames using Matplotlib.\n",
    "\n",
    "    Parameters:\n",
    "    predicted_frames (numpy array): Array of predicted frames.\n",
    "    \"\"\"\n",
    "    num_frames = predicted_frames.shape[0]\n",
    "\n",
    "    # Create a figure to display the frames\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        plt.subplot(1, num_frames, i + 1)  # Create a subplot for each frame\n",
    "        plt.imshow(predicted_frames[i])  # Display the frame\n",
    "        plt.axis('off')  # Hide the axes\n",
    "        plt.title(f'Frame {i}')  # Optional: add title to each frame\n",
    "\n",
    "    plt.tight_layout()  # Adjust subplots to fit in to the figure area.\n",
    "    plt.show()  # Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9c9ee04-e9ac-4b16-be9a-39e894598ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_frames(predicted_frames):\n",
    "    \"\"\"\n",
    "    Create and display an animation from predicted frames in Jupyter Notebook.\n",
    "    \n",
    "    Parameters:\n",
    "    predicted_frames (numpy array): Array of predicted frames, expected shape (num_frames, 128, 128, 3).\n",
    "    \"\"\"\n",
    "    num_frames = predicted_frames.shape[0]\n",
    "\n",
    "    # Create a figure\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Display the first frame initially\n",
    "    img = ax.imshow(predicted_frames[0], animated=True)\n",
    "    ax.axis('off')  # Hide the axis for visual appeal\n",
    "\n",
    "    # Update function for the animation\n",
    "    def update(frame_num):\n",
    "        img.set_array(predicted_frames[frame_num])  # Update the image data with the new frame\n",
    "        return [img]\n",
    "\n",
    "    # Create the animation: FuncAnimation creates a new image every interval\n",
    "    ani = FuncAnimation(\n",
    "        fig, update, frames=num_frames, interval=200, blit=True  # interval sets the frame display time (in ms)\n",
    "    )\n",
    "\n",
    "    # Display the animation in Jupyter\n",
    "    plt.close(fig)  # Close the figure to prevent a static image from being displayed\n",
    "    return HTML(ani.to_jshtml())  # Display the animation in HTML format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beb5de3d-bc13-47c1-b83d-db212ea6c98a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"decoder_output\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [512, 512, 27], output_shape = [9, 128, 128, 3]\n\nCall arguments received by layer \"decoder_output\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 512, 512, 27), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m output_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m  \u001b[38;5;66;03m# Number of frames to predict\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Build the model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Print model summary\u001b[39;00m\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(input_shape, output_frames)\u001b[0m\n\u001b[1;32m     31\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mConv2D(output_frames \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)  \u001b[38;5;66;03m# (128, 128, output_frames * 3)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Reshape to (output_frames, 128, 128, 3)\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecoder_output\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Build and compile the model\u001b[39;00m\n\u001b[1;32m     37\u001b[0m model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mModel(encoder_input, decoder_output, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdilated_frame_predictor\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.8/site-packages/keras/src/layers/reshaping/reshape.py:118\u001b[0m, in \u001b[0;36mReshape._fix_unknown_dimension\u001b[0;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[1;32m    116\u001b[0m     output_shape[unknown] \u001b[38;5;241m=\u001b[39m original \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m known\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m original \u001b[38;5;241m!=\u001b[39m known:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"decoder_output\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [512, 512, 27], output_shape = [9, 128, 128, 3]\n\nCall arguments received by layer \"decoder_output\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 512, 512, 27), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# Define input shape for 128x128x3 images\n",
    "input_shape = (128, 128, 3)  # Height, Width, Channels\n",
    "output_frames = 9  # Number of frames to predict\n",
    "\n",
    "# Build the model\n",
    "model = build_model(input_shape, output_frames)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779d7a75-3a14-40c3-b332-ef34bdc59623",
   "metadata": {},
   "outputs": [],
   "source": [
    "sprite_sheet_paths = ['/Users/lofichill/Desktop/NIPA/a-yo-image/initial-experiments/tomato/image folder/City_men_1_10f/Walk.png',\n",
    "                      '/Users/lofichill/Desktop/NIPA/a-yo-image/initial-experiments/tomato/image folder/City_men_2_10f/Walk.png',\n",
    "                      '/Users/lofichill/Desktop/NIPA/a-yo-image/initial-experiments/tomato/image folder/City_men_3_10f/Walk.png',\n",
    "                      '/Users/lofichill/Desktop/NIPA/a-yo-image/initial-experiments/tomato/image folder/Gangsters_1_10f/Walk.png',\n",
    "                      '/Users/lofichill/Desktop/NIPA/a-yo-image/initial-experiments/tomato/image folder/Gangsters_2_10f/Walk.png',\n",
    "                      '/Users/lofichill/Desktop/NIPA/a-yo-image/initial-experiments/tomato/image folder/Gangsters_3_10f/Walk.png'\n",
    "                     ] \n",
    "num_frames = 10  # Number of frames in the sprite sheet\n",
    "X, Y = process_sprite_data(sprite_sheet_paths, num_frames)\n",
    "\n",
    "print(f\"X shape: {X[1:].shape}\")  # Should be (num_sequences, 128, 128, 3)\n",
    "print(f\"Y shape: {Y[1:].shape}\")  # Should be (num_sequences, 9, 128, 128, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7133de22-7ec4-4c5c-ac03-5ff4e6abb035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model without the first example\n",
    "model.fit(X[1:], Y[1:], epochs=50, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a54c95d-6c26-4c66-ab13-e7ab597c4f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4630210a-f033-4a7e-b251-ae495369b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_frames = model.predict(np.expand_dims(X[0], axis=0))  # Add batch dimension\n",
    "\n",
    "# Post-process the predicted frames if necessary\n",
    "predicted_frames = predicted_frames.squeeze()  # Remove the batch dimension if needed\n",
    "print(f\"Predicted frames shape: {predicted_frames.shape}\")  # Should be (9, 128, 128, 3)\n",
    "\n",
    "# Optionally, save the predicted frames or display them\n",
    "for i in range(predicted_frames.shape[0]):\n",
    "    frame = (predicted_frames[i] * 255).astype(np.uint8)  # Convert back to [0, 255]\n",
    "    # Image.fromarray(frame).save(f\"predicted_frame_{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604881f5-1752-419d-87d8-5380ea9bba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_frames = merge_frames(X[0], predicted_frames)\n",
    "print(\"Merged shape:\", merged_frames.shape)  # Expected Output: (10, 128, 128, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a7d5b-b1f4-45ef-a3e3-bd0f37d65ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_predicted_frames(merged_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4075742-88b0-4a6b-8277-3b8827bde35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_frames(merged_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f782748-adcc-47f9-87ba-074d07990e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # saving gif file\n",
    "\n",
    "# def animate_frames(predicted_frames, save_path=None, fps=5):\n",
    "#     \"\"\"\n",
    "#     Create, display, and optionally save an animation from predicted frames in Jupyter Notebook.\n",
    "    \n",
    "#     Parameters:\n",
    "#     predicted_frames (numpy array): Array of predicted frames, expected shape (num_frames, 128, 128, 3).\n",
    "#     save_path (str, optional): Path to save the animation (supports formats like .mp4 or .gif).\n",
    "#     fps (int, optional): Frames per second for the animation. Default is 5 fps.\n",
    "#     \"\"\"\n",
    "#     num_frames = predicted_frames.shape[0]\n",
    "\n",
    "#     # Create a figure\n",
    "#     fig, ax = plt.subplots()\n",
    "\n",
    "#     # Display the first frame initially\n",
    "#     img = ax.imshow(predicted_frames[0], animated=True)\n",
    "#     ax.axis('off')  # Hide the axis for visual appeal\n",
    "\n",
    "#     # Update function for the animation\n",
    "#     def update(frame_num):\n",
    "#         img.set_array(predicted_frames[frame_num])  # Update the image data with the new frame\n",
    "#         return [img]\n",
    "\n",
    "#     # Create the animation: FuncAnimation creates a new image every interval\n",
    "#     ani = FuncAnimation(\n",
    "#         fig, update, frames=num_frames, interval=1000//fps, blit=True  # interval in ms\n",
    "#     )\n",
    "\n",
    "#     if save_path:\n",
    "#         # Save the animation\n",
    "#         if save_path.endswith(\".mp4\"):\n",
    "#             ani.save(save_path, writer=\"ffmpeg\", fps=fps)\n",
    "#         elif save_path.endswith(\".gif\"):\n",
    "#             ani.save(save_path, writer=\"pillow\", fps=fps)\n",
    "#         print(f\"Animation saved to {save_path}\")\n",
    "    \n",
    "#     # Display the animation in Jupyter\n",
    "#     plt.close(fig)  # Close the figure to prevent a static image from being displayed\n",
    "#     return HTML(ani.to_jshtml())\n",
    "\n",
    "# animate_frames(merged_frames, '/Users/lofichill/Desktop/NIPA/a-yo-image/initial-experiments/tomato/output/Stride_Conv.gif', fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc52b81e-7693-421a-ab7e-23c815ad32dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
